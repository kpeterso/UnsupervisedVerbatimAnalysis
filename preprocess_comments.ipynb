{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from gensim import corpora, models\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f=open(\"Data/comments.txt\",'r')\n",
    "docSet = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "added_stop=[\"car\",\"cars\", \"nissan\", \"altima\", \"sentra\", \"sentras\", \"vehicle\", \"vehicles\", \"get\", \"s\", \\\n",
    "            \"really\", \"new\", \"rogue\", \"rouge\", \"pathfinder\", \"maxima\", \"versa\", \"murano\", \"le\", \"sl\", \\\n",
    "            \"sv\", \"yet\", \"n\", \"honda\", \"etc\", \"xterra\", \"r\", \"u\", \"ml\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-5cf795c8198d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRegexpTokenizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'\\w+'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;31m#create a list of stop words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mstop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"english\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mstops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0madded_stopwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'words'"
     ]
    }
   ],
   "source": [
    "# Create p_stemmer of class PorterStemmer\n",
    "#p_stemmer = PorterStemmer()\n",
    "#SBstemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer=WordNetLemmatizer()\n",
    "#create a tokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#create a list of stop words\n",
    "#stop = stopwords.words(\"english\")\n",
    "stops=set(stopwords.words(\"english\")+added_stopwords)\n",
    "\n",
    "token_texts=[]\n",
    "non_english=0\n",
    "for i in docSet:\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    #remove non-letter chars\n",
    "    remove_dash=raw.replace(\"-\",\"\").replace(\"don\\'t know\", \"dont_know\").replace(\"responce\",\"response\").\\\n",
    "        replace(\" u \", \" you \").replace(\"a bit\",\"\").replace(\"a/c\",\"air_conditioning\")\n",
    "        \n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", remove_dash)\n",
    "    #tokenize words\n",
    "    tokens = tokenizer.tokenize(letters_only)\n",
    "    token_texts.append(tokens)\n",
    "        \n",
    "bigrams=models.phrases.Phrases(token_texts)\n",
    "trigrams=models.phrases.Phrases(bigrams[token_texts])\n",
    "bi_texts=trigrams[bigrams[token_texts]]\n",
    "texts=[]\n",
    "for i in bi_texts:\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [j for j in i if not j in stops]\n",
    "    \n",
    "    # stem tokens\n",
    "    #stemmed_tokens = [SBstemmer.stem(j) for j in stopped_tokens]\n",
    "    lemmad_tokens = [lemmatizer.lemmatize(j) for j in stopped_tokens]\n",
    "    \n",
    "    # add tokenized document to list\n",
    "    texts.append(lemmad_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "docSet2=[]\n",
    "for doc in docSet:\n",
    "    doc2=doc.decode('utf-8','ignore').encode(\"utf-8\").lower()\n",
    "    doc2=doc2.replace(\"pick-up\",\"pickup\")\n",
    "    doc2=doc2.replace(\".\",\" [dot] \")\n",
    "    doc2=doc2.replace(\",\",\" [comma] \")\n",
    "    doc2=doc2.replace(\"!\",\" [xclm] \")\n",
    "    doc2=doc2.replace(\"-\",\" [dash] \")\n",
    "    doc2=doc2.replace(\"$\",\" [dlr] \")\n",
    "    doc2=doc2.replace(\"/\", \"\")\n",
    "    doc2=doc2.replace(\"?\",\" [qmark] \")\n",
    "    doc2=doc2.replace(\"(\", \"\")\n",
    "    doc2=doc2.replace(\")\",\"\")\n",
    "    docSet2.append(doc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g=open(\"Data/p_comments.txt\",'w')\n",
    "for item in docSet2:\n",
    "  g.write(\"%s\" % item)\n",
    "g.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have been driving a nissan for 3 years without any issues.\r\n",
      "\n",
      "['i', 'have', 'been', 'driving', 'a', 'nissan', 'for', '3', 'years', 'without', 'any', 'issues', '[dot]']\n"
     ]
    }
   ],
   "source": [
    "i=random.randint(0,150000)\n",
    "print(docSet[i])\n",
    "print(docSet2[i].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
